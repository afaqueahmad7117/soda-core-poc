{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters To Be Passed\n",
    "- File path / Table Name\n",
    "- `checks.yml` path\n",
    "- `configuration.yml` path\n",
    "- Name of view to be created (should be same as the one specified in `checks.yml`)\n",
    "- Data source to refer to (should be same as the one defined in `configuration.yml`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/24 16:54:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_p = \"data/supestore_orders.csv\"\n",
    "returned_orders_p = \"data/supestore_returns.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = spark.read.csv(orders_p, header=True, inferSchema=True, sep=\"|\")\n",
    "df_returned_orders = spark.read.csv(returned_orders_p, header=True, inferSchema=True, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------+----------+------------+-----------+-------------+--------+-------------+---------+--------+-----------+------+---------------+---------+------------+--------------------+-----------------+--------+--------+------------------+\n",
      "|row_id|      order_id|order_date| ship_date|   ship_mode|customer_id|customer_name| segment|      country|     city|   state|postal_code|region|     product_id| category|sub_category|        product_name|            sales|quantity|discount|            profit|\n",
      "+------+--------------+----------+----------+------------+-----------+-------------+--------+-------------+---------+--------+-----------+------+---------------+---------+------------+--------------------+-----------------+--------+--------+------------------+\n",
      "|     1|CA-2018-152156|2018-11-08|2018-11-07|Second Class|   CG-12520|  Claire Gute|Consumer|United States|Henderson|Kentucky|    42420.0| South|FUR-BO-10001798|Furniture|   Bookcases|Bush Somerset Col...|           261.96|       2|     0.0|           41.9136|\n",
      "|     2|CA-2018-152156|2018-11-08|2018-11-11|Second Class|   CG-12520|  Claire Gute|Consumer|United States|Henderson|Kentucky|    42420.0| South|FUR-CH-10000454|Furniture|      Chairs|Hon Deluxe Fabric...|731.9399999999999|       3|     0.0|219.58199999999997|\n",
      "+------+--------------+----------+----------+------------+-----------+-------------+--------+-------------+---------+--------+-----------+------+---------------+---------+------------+--------------------+-----------------+--------+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|returned|      order_id|\n",
      "+--------+--------------+\n",
      "|     Yes|CA-2016-100762|\n",
      "|     Yes|CA-2016-100762|\n",
      "+--------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_returned_orders.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders.createOrReplaceTempView(\"orders_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_p = \"configuration.yml\"\n",
    "checks_p = \"checks/orders.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soda.scan import Scan\n",
    "\n",
    "scan = Scan()\n",
    "\n",
    "scan.set_data_source_name(\"spark_df\")\n",
    "scan.add_configuration_yaml_file(config_p)\n",
    "scan.add_sodacl_yaml_file(checks_p)\n",
    "scan.add_spark_session(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=31067Kb max_used=31079Kb free=100004Kb\n",
      " bounds [0x000000010f1e8000, 0x0000000111068000, 0x00000001171e8000]\n",
      " total_blobs=11383 nmethods=10425 adapters=866\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO   | Soda Core 3.3.18\n",
      "INFO   | Scan summary:\n",
      "INFO   | 4/7 checks PASSED: \n",
      "INFO   |     orders_vw in spark_df\n",
      "INFO   |       missing_count(row_id) = 0 [PASSED]\n",
      "INFO   |       duplicate_count(row_id) = 0 [PASSED]\n",
      "INFO   |       missing_count(order_id) = 0 [PASSED]\n",
      "INFO   |       missing_count(order_date) = 0 [PASSED]\n",
      "INFO   | 3/7 checks FAILED: \n",
      "INFO   |     orders_vw in spark_df\n",
      "INFO   |       row_count > 10000 [FAILED]\n",
      "INFO   |         check_value: 9993\n",
      "INFO   |       count_order_date_after_ship_date = 0 [FAILED]\n",
      "INFO   |         check_value: 1.0\n",
      "INFO   |       Schema Check [FAILED]\n",
      "INFO   |         fail_missing_column_names = [year]\n",
      "INFO   |         schema_measured = [row_id int, order_id string, order_date date, ship_date date, ship_mode string, customer_id string, customer_name string, segment string, country string, city string, state string, postal_code double, region string, product_id string, category string, sub_category string, product_name string, sales double, quantity int, discount double, profit double]\n",
      "INFO   | Oops! 3 failures. 0 warnings. 0 errors. 4 pass.\n"
     ]
    }
   ],
   "source": [
    "print(scan.get_logs_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definitionName': None,\n",
       " 'defaultDataSource': 'spark_df',\n",
       " 'dataTimestamp': '2024-08-24T11:24:52+00:00',\n",
       " 'scanStartTimestamp': '2024-08-24T11:24:52+00:00',\n",
       " 'scanEndTimestamp': '2024-08-24T11:24:53+00:00',\n",
       " 'hasErrors': False,\n",
       " 'hasWarnings': False,\n",
       " 'hasFailures': True,\n",
       " 'metrics': [{'identity': 'metric-spark_df-orders_vw-schema',\n",
       "   'metricName': 'schema',\n",
       "   'dataSourceName': 'spark_df',\n",
       "   'tableName': 'orders_vw',\n",
       "   'partitionName': None,\n",
       "   'columnName': None,\n",
       "   'value': [{'columnName': 'row_id', 'sourceDataType': 'int'},\n",
       "    {'columnName': 'order_id', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'order_date', 'sourceDataType': 'date'},\n",
       "    {'columnName': 'ship_date', 'sourceDataType': 'date'},\n",
       "    {'columnName': 'ship_mode', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'customer_id', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'customer_name', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'segment', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'country', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'city', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'state', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'postal_code', 'sourceDataType': 'double'},\n",
       "    {'columnName': 'region', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'product_id', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'category', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'sub_category', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'product_name', 'sourceDataType': 'string'},\n",
       "    {'columnName': 'sales', 'sourceDataType': 'double'},\n",
       "    {'columnName': 'quantity', 'sourceDataType': 'int'},\n",
       "    {'columnName': 'discount', 'sourceDataType': 'double'},\n",
       "    {'columnName': 'profit', 'sourceDataType': 'double'}]},\n",
       "  {'identity': 'metric-spark_df-orders_vw-order_id-missing_count',\n",
       "   'metricName': 'missing_count',\n",
       "   'value': 0,\n",
       "   'dataSourceName': 'spark_df'},\n",
       "  {'identity': 'metric-spark_df-orders_vw-row_id-missing_count',\n",
       "   'metricName': 'missing_count',\n",
       "   'value': 0,\n",
       "   'dataSourceName': 'spark_df'},\n",
       "  {'identity': 'metric-spark_df-orders_vw-row_id-duplicate_count',\n",
       "   'metricName': 'duplicate_count',\n",
       "   'value': 0,\n",
       "   'dataSourceName': 'spark_df'},\n",
       "  {'identity': 'metric-spark_df-orders_vw-order_date-missing_count',\n",
       "   'metricName': 'missing_count',\n",
       "   'value': 0,\n",
       "   'dataSourceName': 'spark_df'},\n",
       "  {'identity': 'metric-spark_df-orders_vw-row_count',\n",
       "   'metricName': 'row_count',\n",
       "   'value': 9993,\n",
       "   'dataSourceName': 'spark_df'},\n",
       "  {'identity': 'metric-spark_df-count_order_date_after_ship_date = 0-511797ff',\n",
       "   'metricName': 'count_order_date_after_ship_date = 0',\n",
       "   'value': 1.0,\n",
       "   'dataSourceName': 'spark_df'}],\n",
       " 'checks': [{'identity': 'abec231b',\n",
       "   'name': 'row_count > 10000',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  row_count > 10000',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 3, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': None,\n",
       "   'metrics': ['metric-spark_df-orders_vw-row_count'],\n",
       "   'outcome': 'fail',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 9993,\n",
       "    'fail': {'lessThanOrEqual': 10000.0}}},\n",
       "  {'identity': 'b00a1b94',\n",
       "   'name': 'count_order_date_after_ship_date = 0',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  - count_order_date_after_ship_date = 0:\\n      count_order_date_after_ship_date query: |\\n        SELECT COUNT(*) \\n        FROM orders_vw \\n        WHERE order_date > ship_date\\n\\n# Schema validation\\n',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 10, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': None,\n",
       "   'metrics': ['metric-spark_df-count_order_date_after_ship_date = 0-511797ff'],\n",
       "   'outcome': 'fail',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 1.0,\n",
       "    'fail': {'greaterThan': 0.0, 'lessThan': 0.0}}},\n",
       "  {'identity': '8709d35d',\n",
       "   'name': 'Schema Check',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  - schema:\\n      fail:\\n        when required column missing:\\n          - order_id\\n          - year\\n        when wrong column type:\\n          order_id: TEXT\\n',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 17, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': None,\n",
       "   'metrics': ['metric-spark_df-orders_vw-schema'],\n",
       "   'outcome': 'fail',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [{'type': 'csv',\n",
       "      'text': 'Column,Type\\nrow_id,int\\norder_id,string\\norder_date,date\\nship_date,date\\nship_mode,string\\ncustomer_id,string\\ncustomer_name,string\\nsegment,string\\ncountry,string\\ncity,string\\nstate,string\\npostal_code,double\\nregion,string\\nproduct_id,string\\ncategory,string\\nsub_category,string\\nproduct_name,string\\nsales,double\\nquantity,int\\ndiscount,double\\nprofit,double',\n",
       "      'title': 'Schema'},\n",
       "     {'type': 'csv',\n",
       "      'text': 'Column,Event,Details\\nyear,:icon-fail: Required Column Missing, \\n',\n",
       "      'title': 'Diagnostics'}],\n",
       "    'column_additions': [],\n",
       "    'column_deletions': [],\n",
       "    'column_index_changes': {},\n",
       "    'column_index_mismatches': {},\n",
       "    'column_type_changes': {},\n",
       "    'column_type_mismatches': {},\n",
       "    'missing_column_names': ['year'],\n",
       "    'present_column_names': [],\n",
       "    'preferredChart': 'bars',\n",
       "    'valueLabel': '1 schema event(s)',\n",
       "    'valueSeries': {'values': [{'label': 'fail',\n",
       "       'value': 1,\n",
       "       'outcome': 'fail'},\n",
       "      {'label': 'warn', 'value': 0, 'outcome': 'warn'},\n",
       "      {'label': 'pass', 'value': 20, 'outcome': 'pass'}]}}},\n",
       "  {'identity': 'e96b703a',\n",
       "   'name': 'missing_count(row_id) = 0',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  missing_count(row_id) = 0',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 4, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': 'row_id',\n",
       "   'metrics': ['metric-spark_df-orders_vw-row_id-missing_count'],\n",
       "   'outcome': 'pass',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 0,\n",
       "    'fail': {'greaterThan': 0.0, 'lessThan': 0.0}}},\n",
       "  {'identity': '5169048b',\n",
       "   'name': 'duplicate_count(row_id) = 0',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  duplicate_count(row_id) = 0',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 5, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': 'row_id',\n",
       "   'metrics': ['metric-spark_df-orders_vw-row_id-duplicate_count'],\n",
       "   'outcome': 'pass',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 0,\n",
       "    'fail': {'greaterThan': 0.0, 'lessThan': 0.0}}},\n",
       "  {'identity': 'b62f88a7',\n",
       "   'name': 'missing_count(order_id) = 0',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  missing_count(order_id) = 0',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 6, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': 'order_id',\n",
       "   'metrics': ['metric-spark_df-orders_vw-order_id-missing_count'],\n",
       "   'outcome': 'pass',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 0,\n",
       "    'fail': {'greaterThan': 0.0, 'lessThan': 0.0}}},\n",
       "  {'identity': '82ac4d5a',\n",
       "   'name': 'missing_count(order_date) = 0',\n",
       "   'type': 'generic',\n",
       "   'definition': 'checks for orders_vw:\\n  missing_count(order_date) = 0',\n",
       "   'resourceAttributes': [],\n",
       "   'location': {'filePath': 'checks/orders.yml', 'line': 7, 'col': 5},\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'filter': None,\n",
       "   'column': 'order_date',\n",
       "   'metrics': ['metric-spark_df-orders_vw-order_date-missing_count'],\n",
       "   'outcome': 'pass',\n",
       "   'outcomeReasons': [],\n",
       "   'archetype': None,\n",
       "   'diagnostics': {'blocks': [],\n",
       "    'value': 0,\n",
       "    'fail': {'greaterThan': 0.0, 'lessThan': 0.0}}}],\n",
       " 'queries': [{'name': '1.spark_df.orders_vw.aggregation[0]',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'partition': None,\n",
       "   'column': None,\n",
       "   'sql': 'SELECT \\n  COUNT(*),\\n  COUNT(CASE WHEN row_id IS NULL THEN 1 END),\\n  COUNT(CASE WHEN order_id IS NULL THEN 1 END),\\n  COUNT(CASE WHEN order_date IS NULL THEN 1 END) \\nFROM orders_vw',\n",
       "   'exception': None,\n",
       "   'duration': '0:00:00.396977'},\n",
       "  {'name': '4.spark_df.orders_vw.row_id.duplicate_count',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'partition': None,\n",
       "   'column': 'row_id',\n",
       "   'sql': '\\nWITH frequencies AS (\\n    SELECT COUNT(*) AS frequency\\n    FROM orders_vw\\n    WHERE row_id IS NOT NULL\\n    GROUP BY row_id)\\nSELECT COUNT(*)\\nFROM frequencies\\nWHERE frequency > 1',\n",
       "   'exception': None,\n",
       "   'duration': '0:00:00.454552'},\n",
       "  {'name': '4.spark_df.orders_vw.row_id.duplicate_count.failing_sql',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'partition': None,\n",
       "   'column': 'row_id',\n",
       "   'sql': '\\nWITH frequencies AS (\\n    SELECT row_id\\n    FROM orders_vw\\n    WHERE row_id IS NOT NULL\\n    GROUP BY row_id\\n    HAVING COUNT(*) > 1)\\nSELECT main.*\\nFROM orders_vw main\\nJOIN frequencies ON main.row_id = frequencies.row_id\\n',\n",
       "   'exception': None,\n",
       "   'duration': None},\n",
       "  {'name': '4.spark_df.orders_vw.row_id.duplicate_count.passing_sql',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'partition': None,\n",
       "   'column': 'row_id',\n",
       "   'sql': '\\nWITH frequencies AS (\\n    SELECT row_id\\n    FROM orders_vw\\n    WHERE row_id IS NOT NULL\\n    GROUP BY row_id\\n    HAVING COUNT(*) <= 1)\\nSELECT main.*\\nFROM orders_vw main\\nJOIN frequencies ON main.row_id = frequencies.row_id\\n',\n",
       "   'exception': None,\n",
       "   'duration': None},\n",
       "  {'name': '2.spark_df.user_defined_query[count_order_date_after_ship_date = 0]',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': None,\n",
       "   'partition': None,\n",
       "   'column': None,\n",
       "   'sql': 'SELECT COUNT(*) \\nFROM orders_vw \\nWHERE order_date > ship_date',\n",
       "   'exception': None,\n",
       "   'duration': '0:00:00.112496'},\n",
       "  {'name': '3.spark_df.orders_vw.schema[orders_vw]',\n",
       "   'dataSource': 'spark_df',\n",
       "   'table': 'orders_vw',\n",
       "   'partition': None,\n",
       "   'column': None,\n",
       "   'sql': 'DESCRIBE orders_vw',\n",
       "   'exception': None,\n",
       "   'duration': '0:00:00.234933'}],\n",
       " 'automatedMonitoringChecks': [],\n",
       " 'profiling': [],\n",
       " 'metadata': [],\n",
       " 'logs': [{'level': 'INFO',\n",
       "   'message': 'Soda Core 3.3.18',\n",
       "   'timestamp': '2024-08-24T11:24:52+00:00',\n",
       "   'index': 1,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': 'Scan summary:',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 2,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '4/7 checks PASSED: ',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 3,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '    orders_vw in spark_df',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 4,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      missing_count(row_id) = 0 [PASSED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 5,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      duplicate_count(row_id) = 0 [PASSED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 6,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      missing_count(order_id) = 0 [PASSED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 7,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      missing_count(order_date) = 0 [PASSED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 8,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '3/7 checks FAILED: ',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 9,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '    orders_vw in spark_df',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 10,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      row_count > 10000 [FAILED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 11,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '        check_value: 9993',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 12,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      count_order_date_after_ship_date = 0 [FAILED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 13,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '        check_value: 1.0',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 14,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '      Schema Check [FAILED]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 15,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '        fail_missing_column_names = [year]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 16,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': '        schema_measured = [row_id int, order_id string, order_date date, ship_date date, ship_mode string, customer_id string, customer_name string, segment string, country string, city string, state string, postal_code double, region string, product_id string, category string, sub_category string, product_name string, sales double, quantity int, discount double, profit double]',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 17,\n",
       "   'doc': None,\n",
       "   'location': None},\n",
       "  {'level': 'INFO',\n",
       "   'message': 'Oops! 3 failures. 0 warnings. 0 errors. 4 pass.',\n",
       "   'timestamp': '2024-08-24T11:24:53+00:00',\n",
       "   'index': 18,\n",
       "   'doc': None,\n",
       "   'location': None}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan.get_scan_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sodapoc311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
